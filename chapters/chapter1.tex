\chapter{Introduction}
\label{chap:one}

Neuroscience is undergoing a data-driven revolution.  In
laboratories around the world, optical tools are illuminating
thousands of neurons at a time \citep{kerr2008imaging}.
% grewe2010high, prevedel2014simultaneous, ahrens2013whole, lemon2015whole}.  
For some organisms, we can now monitor large fractions of the neurons
in the brain \citep{ahrens2013whole, prevedel2014simultaneous,
  lemon2015whole}.  Not long ago, this immense recording capability
would have been mind-boggling to the average practitioner poking
around in the dark, hoping to record from a handful of neurons.
Complementing these recording capabilities, recent advances in
connectomics --- the mapping of synaptic connectivity
\citep{sporns2005human} --- are providing a more complete atlas of the
wiring diagram that underlies neural activity
\citep{lichtman2008technicolour, helmstaedter2013connectomic,
  oh2014mesoscale}.  Likewise, sophisticated monitoring and processing
methods \citep[e.g.][]{berman2014mapping, wiltschko2015mapping} are providing rich
measurements of the natural behavior that this activity gives rise to.

% Paradigm shift -> data-driven science
% new datasets contain patterns that can expose where existing theories fall short
% new data contains patterns that can suggest new hypotheses, model additions
These unprecedented technological
capabilities are leading to a fundamental paradigm shift.
The scientific process of proposing hypotheses, testing them against
experimental observations, and revising them accordingly, is becoming increasingly
data-driven.  Rather than collecting data to test a specific
hypothesis, we can now collect large-scale recordings in relatively
unstructured experimental setups (e.g. from freely behaving animals)
and use statistical structure in the data to guide us toward 
hypotheses. Of course, there is no ``free lunch'' --- we must still
specify what types of structure to look for. For
example, when we cluster neuronal data we are hypothesizing
that there are different types of neurons, and when we apply principal
components analysis to neural population recordings we are 
assuming that neural activity reflects a low-dimensional latent state.
As we move toward this more data-driven paradigm, we are presented
with a challenge: how do we formulate models that capture
our intuitions and hypotheses about neural computation, how do we
fit these models to large scale recordings, and how do we revise 
our models based on what we have learned?

This thesis builds a sequence of modeling and inference tools to
address this challenge. We build tools for analyzing \emph{neural spike
  trains}, which are sequences of discrete events in time, and we use
probabilistic models to compose flexible hypotheses that allow us to
\emph{discover structure} in the dynamics of spike trains.  The
workhorse of this process is a set of \emph{Bayesian methods},
statistical inference algorithms that take in a neural recording and
output a distribution over parameters and variables of our model.  We
begin by describing each of these these in turn.


% What is a neural spike train?
\section{Neurons, Spikes, and Computation}
The human brain is a three pound ball of densely packed cells.  It is
soft, with a consistency like that of tofu.  Amazingly, from this
curd-like mass of cells, our mental faculties arise.  About half of
the roughly 170 billion cells in our brain are neurons, widely
believed to be the fundamental units of computation
\citep{DayanAbbott}.  Neurons are electrically excitable cells that
contain an ionic soup of charged atoms like sodium, potassium, and
calcium, which together maintain a difference in electrical potential
between the inside and the outside of the cell membrane
\citep{kandel2000principles}.
%In most neurons, when the membrane potential is excited
%above a certain threshold, a cascade of ion channels opens and closes,
%causing the membrane potential to undergo a characteristic action
%potential, or ``spike.''

In most neurons, the cell membrane is littered with voltage-gated ion
channels. At rest, these channels are closed, but if the membrane
potential is excited above a certain threshold, some channels
rapidly start to open, initiating an action potential.
As described in the seminal work of \cite{hodgkin1952quantitative},
positively charged sodium ions are first to rush into the cell,
causing a further increase in membrane potential and leading more
channels to open.
%This excitatory feedback loop continues until all
%sodium channels have opened.
The upswing in membrane potential
eventually causes a reversal in cell polarity and the inactivation of
sodium channels. At the same time, potassium channels open, allowing
an efflux of positively charged potassium ions. Together, these halt the rising membrane
potential and drive it back down toward its resting state.
This brief action potential, or ``spike,'' takes place in
just a few milliseconds.

% Spike hypothesis: at the heart of this computation is the action potential,
% or ``spike.'' These spikes are
The first challenge of deciphering neural computation is understanding
how information is encoded in patterns of coordinated spiking activity
across neural populations. To first approximation, spikes are
discrete events in time.  That is, in a short enough
window of time, a neuron either does or does not fire an action
potential.  Much progress has been made in understanding how sensory
neurons digitize continuous signals and how the activity of motor
neurons innervates muscles and drives behavior \citep{rieke1999spikes}, yet how internal
states, plans, goals, decisions, and thoughts are encoded remains
largely a mystery.


The second challenge is understanding how this information is
transformed over time. The neurons in our brain are connected by
about~$10^{14}$ synapses \citep{kandel2000principles}. When a
pre-synaptic neuron fires an action potential, neurotransmitters are
released that then bind with receptors on the post-synaptic neuron,
causing its ion channels to open and allowing current to flow into or
out of the post-synaptic cell. These currents induce brief changes in
the membrane potential of the downstream cell called post-synaptic
potentials (PSPs). Depending on the direction of current, the PSP will
be either excitatory, making the downstream neuron more likely to
spike, or inhibitory, suppressing post-synaptic spiking. The 
probability of neurotransmitter release and the number of post-synaptic 
receptors together determine the ``strength'' of the synapse, 
which is manifest in the amplitude of the PSP \citep{cowan2003synapses}. 


This web of synaptic connections imbues neural circuits with complex
dynamics that govern how patterns of neural activity evolve over time,
along with the information they encode. It is these dynamics that actually
perform computation --- the transformation of an input into an output.
While these dynamics have been very well studied at the level of
single connections between pairs of neurons, the dynamics of large
networks of interconnected neurons is at the frontier of research.

% Learning and plasticity
Perhaps the most fundamental aspect of intelligence is our ability to
learn, to store memories, and to generalize from past experience.  In
neural circuits, learning is most directly associated with the process
of synaptic plasticity. In response to coordinated patterns of pre-
and post-synaptic spiking, synapses adapt their strength. This synaptic
plasticity leads to changes in the dynamics of neural circuits.  In
some cases, this plasticity leads to the creation of new associations
that support generalization from noisy or partial sensory input.
Thus, a third challenge of deciphering neural computation is
understanding the processes of learning that cause neural dynamics to
evolve in an activity-dependent manner.

Our goal, as neuroscientists, is to address these three challenges ---
encoding, dynamics, and learning --- by searching for patterns in
large-scale recordings of neural spike trains, simultaneously recorded
time series of spiking activity in populations of neurons. While this
type of data is very hard to collect in humans, it is widely collected
in worms, fish, flies, mice, rats, monkeys, and a host of other model
organisms.  Still, these model organisms have taught us a great deal
about the workings of human brains and intelligence.  This thesis does
not focus on the unique aspects of any single organism or brain
region, but rather on the general challenges associated with modeling
structure in time series of discrete events that are common to all
neural spike trains.


\section{Discovering Structure: Types, Features, Networks, and States}
% Identifying levels of abstraction
% - Marr levels of analysis
Discovering structure is primarily about finding meaningful abstractions.  
While neurons, spikes, and
synapses are the elementary building blocks of many models of neural
computation, our goal is to connect this level of detail to more
abstract theories of computation.  Just as our knowledge of
\emph{in silico} computation is partitioned into a hierarchy of
concepts --- from transistors, logic gates, pipelines and processors,
to assembly code, operating systems, algorithms and programs --- our
knowledge of neural computation must also include a hierarchy of
concepts and descriptions.  \citet{marr1982vision} proposed three
levels of abstraction: the computational level, which concerns the inputs
and outputs of a system; the algorithmic level, which specifies the
transformations between inputs and outputs; and the implementation
level, which focuses on how these transformations are realized in
neural substrates.  From this perspective, our goal is to interpolate
between neural spike trains, typically associated with implementation
level descriptions, and higher level abstractions, like those
hypothesized by algorithmic and computational theories. To do so,
we compose our spike train models out of a set of simple
motifs that appear over and over again in models of
neural computation. 

\subsection{Discrete Types and Low-Dimensional Features}
Many successes of neuroscience have come from careful cataloging of
neural types and features.  From the earliest investigations of
Ram\'on y Cajal, it has been clear that the brain is made up of
anatomically distinct cell types \citep{cajal1899textura}, but further
classifications have been made on the basis of functional distinctions
as well. \citet{kuffler1953discharge} identified the response
properties of ``on'' and ``off'' ganglion cells in the retina,
characterizing the ways in which these cells respond to light. 
Kuffler's students, Hubel and Wiesel, carried on this work,
characterizing simple and complex cells in visual cortex
\citep{hubel1962receptive} and eventually winning the Nobel Prize for
their discoveries.  These are pioneering examples of a common theme in
neuroscience: clustering cells into discrete types that facilitate our
understanding of neural computation. Indeed, the clustering of cells
in the early visual pathway continues to this day
\citep{macosko2015highly, sanes2015types}.

Complementing these classic examples of discrete subtypes of neurons 
are similarly compelling examples of neurons characterized by 
continuous features. Most prominent among these are the place cells 
of the hippocampus \citep{OKeefe78}. These cells fire selectively 
when an animal is in a particular location in its environment. Thus,
each cell is naturally associated with a continuous position in 
space. As we seek to make sense of spike trains from other complex neural circuits, 
discrete latent types and continuous latent features form one of the
core building blocks in our probabilistic modeling toolkit. 


\subsection{Networks and Dynamics}
Networks play a central role in modern neuroscience: they enable us to
reason about complex systems in terms of relationships among their
constituent parts. Whether the nodes of the network represent
individual neurons in a ``connectome''
\citep[e.g.][]{sporns2005human}, populations of cells in a neural
circuit \citep[e.g.][]{felleman1991distributed}, voxels in an fMRI
recording \citep[e.g.][]{friston1994functional}, or idealized neurons
in a theoretical model \citep[e.g.][]{hopfield1982neural}, networks
tell us about the pairwise relationships in large populations of
nodes.  Once we have extracted such a network, a great deal of
intuition can be gleaned from its aggregate properties
\citep{bullmore2009complex, newman2003structure}.  Moreover, the
network itself can often be summarized in terms of latent types and
features of nodes that govern how likely any pair of nodes is to be
connected \citep{Goldenberg-2010}.

We often use networks to represent the dynamics of systems.  For
example, an edge in the network may represent the influence that
activity on one node exerts on the subsequent activity of another. In
this way, networks provide a simple summary of complex dynamics. Much
of this thesis is devoted to learning network representations from
neural spike trains, building on a rich body of existing work on
generalized linear models for neural spike trains
\citep{Paninski-2004, Truccolo-2005, Pillow-2008}.  In these models,
nodes correspond to the individual neurons in our dataset, and the
edges represent a probabilistic relationship between the spiking
activity of one neuron and the future firing rate of its downstream
neighbors.  By combining latent variable models for the network with
dynamics models for its edges, this thesis will construct
sophisticated, hierarchical models for dynamical spike trains.


% Networks play a central role in modern neuroscience: they distill
% complex systems into concise representations.  The goal of
% connectomics \citep{sporns2005human} is to build a wiring diagram, or
% ``connectome,'' of the brain. Each neuron in the brain is a node in
% this network, and the edges correspond to individual synapses
%  from one neuron to another.   Once we have extracted this massive
% network of synaptic connections, we can perform subsequent analyses,
% searching for recurring patterns that will illuminate underlying
% design principles and, hopefully, provide insight into neural
% computation \citep{bullmore2009complex}.

% Networks need not be so fine grained, however.  Systems neuroscientists
% often describe intricate circuits with simplified networks whose nodes
% correspond to populations of cells, and whose edges denote the an
% abundance connections from one population to another
% \citep[e.g.]{felleman1991distributed, scannell1999connectional}.  Since
% neurons in a particular brain region often have highly stereotyped
% patterns of connectivity, it is convenient to describe neural circuits
% in terms of block diagrams.  These abstract networks do not capture
% the nuances of synaptic connections, but they do provide concise
% representations of neural computation.

% Even in functional magnetic resonance imaging (fMRI) studies, where the
% recorded voxels correspond to hundreds of thousands of neurons, networks
% are used to describe \emph{functional connectivity} between brain regions
% \citep{friston1994functional}. While these networks are far removed from
% the underlying synaptic connectivity, they exhibit telling differences
% in healthy and diseased patients \citep{bassett2008hierarchical}
% and show complex structure that is far from random \citep{bassett2006small}.

% % In fact, network models need not be derived from data at all.  Many
% % theoretical models of neural computation use networks to describe
% % idealized interactions between neurons. These interactions are only
% % loosely based on the known properties of neurons and
% % synapses. Nevertheless, these theoretical network models provide
% % invaluable insight into the properties of complex systems
% % \citep{hopfield1982neural, amit1992modeling, van1996chaos, DayanAbbott,
% %   sussillo2009generating}.

\subsection{Latent States of Neural Populations}
Networks model dynamic data in terms of relationships between nodes,
which are typically associated with individual neurons in the
generalized linear model. In some cases, however, it is more natural
to think about neural dynamics in terms of a latent state that evolves
over time but cannot be directly observed. For example, population
activity might reflect a low-dimensional, continuous latent state with
smooth \citep{Yu09} or linear \citep{Smith-2003, paninski2010new}
dynamics, or a discrete latent state with Markovian dynamics
\citep[e.g.][]{jones2007natural, latimer2015single}.  In these models,
the firing rates of individual neurons are a function of the
underlying latent state.

By combining these simple building blocks --- latent types and features, 
networks, and dynamic latent states --- we can express sophisticated 
hypotheses about the underlying structure in observed neural spike trains. 
However, in order to determine this structure, we need to instantiate these 
hypotheses in the form of a model that we can fit and evaluate.
Bayesian probabilistic models and inference algorithms provide a principled 
means of tackling this problem.


\section{A Bayesian Approach} 
Structural hypotheses specify how a set of parameters and latent
variables give rise to patterns of neural activity.  These hypotheses
are naturally formalized in terms of probabilistic generative models,
which provide an intuitive way of expressing the joint probability of
parameters, latent variables, and observed data.  Once formalized, we
use the tools of Bayesian inference to compute the posterior
distribution over parameters and latent variables implied by a given
spike train and our prior beliefs, and use this posterior distribution
to gain insight into the structure of the data and potential
shortcomings of our theories.

Recent decades have witnessed an explosion of interest in Bayesian
machine learning methods, and the development of both theoretical and
analytical tools have led to the widespread adoption of these
techniques \citep{bishop2006pattern, murphy2012probabilistic}.
Nevertheless, performing Bayesian inference in sophisticated models
will always be a challenge. The art of probabilistic modeling lies in
balancing two objectives: designing models that capture the details of
the generative processes underlying our data, while simultaneously
ensuring that these models admit efficient inference algorithms. To
achieve this balance, we often look for model-specific structure that
we can exploit during inference.  This thesis is largely dedicated to
developing algorithms that capitalize on the properties of models in
order to efficiently scale to large recordings.


\begin{figure}[t]
  \centering%
\includegraphics[width=5.5in]{figures/ch1/boxloop} 
\caption[Box's loop]{Box's loop for hypothesis driven probabilistic modeling.
Adapted from~\citet{blei2014build}.}
\label{fig:boxloop}
\end{figure}

Figure~\ref{fig:boxloop} outlines the process of model building.  We
begin by collecting data, in this case, large-scale recordings of
neural spike trains, and building a probabilistic model that captures
our intuitions and hypotheses about structure in that data. Given
these two ingredients, we perform Bayesian inference to compute the
posterior distribution over structural variables and
parameters. Visualizing, analyzing, and exploring this posterior often
lead to new insights and ways to think about generative processes that
can explain further structure in the data. We can also criticize the
model, formally, by assessing goodness-of-fit, evaluating predictive
likelihoods, and performing posterior predictive checks
\citep{Gelman13}. This leads to new hypotheses, which in turn lead to
new models and repetition of this process.  Thus, model building is an
interative process in which we are constantly adapting and refining
hypotheses.  \citet{blei2014build}, from whom this figure has been
adapted, calls this ``Box's loop,'' a reference to the foundational
work of~\citet{box1980sampling}.



We are certainly not the first to adopt a Bayesian approach to
modeling of neural data. Indeed, probabilistic methods
\citep[e.g.][]{brillinger1976identification, Brillinger-1988,
  Paninski-2004, Truccolo-2005, Pillow-2008}, particularly Bayesian
methods \citep[e.g.][]{sahani1999latent, rieke1999spikes, Yu09,
  park2011bayesian, macke2011empirical}, have played a major role in
advancing our understanding of spike trains.  We build on this
foundation and specifically focus on expanding the set of models,
combining standard point process and spike count models with
hierarchical prior distributions that capture latent structure. To go
along with these more sophisticated models, we also derive
efficient\footnote{We do not mean efficient in the formal,
  complexity-theoretic sense.  Indeed, approximate Bayesian inference
  is, in the worst case, NP-hard \citep{dagum1993approximating,
    roth1996hardness}.  We simply mean that the individual iterations
  of our algorithm can be performed in low-order polynomial time and
  that these algorithms yield good empirical results in a reasonable,
  if not provably polynomial, number of iterations.}  Bayesian
inference algorithms that take advantage of clever data augmentation
strategies. Together, these expand the repertoire of probabilistic
models that we may leverage when looking for structure in neural data.


\section{Summary of Contributions}

This thesis consists of a sequence of probabilistic models and
inference algorithms that capture increasingly sophisticated structure
in neural spike trains. While these models build upon one another,
layering in additional structural hypotheses and incorporating more
and more detail, the chapters can also be read independently without
severe loss of clarity, especially by those who are more versed in
machine learning.

We begin with a brief overview of point processes, probabilistic
modeling, and Bayesian inference in Chapter \ref{chap:two}.  This lays
the foundation for the methodological contributions in the remainder
of this thesis. We also introduce common notation for spike trains,
firing rates, and latent variables of the model.  Readers who are well
versed in probabilistic modeling may skip this chapter without much
harm.

Chapter \ref{chap:three} introduces our first probabilistic model for
neural spike trains --- a combination of continuous time Hawkes
processes and probabilistic network models. This is based on
\citet{linderman2014discovering}. However, since this is the first
model of the thesis, we provide a more thorough discussion of network
models, Hawkes processes, and the steps in deriving an efficient
Markov chain Monte Carlo (MCMC) inference algorithm.  This is also the
only chapter that will apply these techniques to spike trains from
domains other than neuroscience, specifically, from finance and
criminology.

Chapter \ref{chap:four} continues the focus on Hawkes processes and
networks, but here we break from the continuous time models of
Chapter~\ref{chap:three} and begin working in discrete time. The
remainder of the thesis continues in this vein.  This chapter also
introduces a scalable variational inference algorithm that leverages
the discrete model structure. This is based on
\citet{linderman2015scalable}.

Chapter~\ref{chap:five} reconsiders the linear interactions of Hawkes
processes and develops a nonlinear autoregressive model for neural
spike trains. Again, we use probabilistic network models as prior
distributions over the pattern of functional interaction, and in doing
so, show that interesting representations of neural populations can
be learned directly from the data. The key to efficient inference is
the \polyagamma augmentation --- a recently developed method for
performing Bayesian inference in models with linear Gaussian structure
and discrete observations \citep{polson2013bayesian}. 
A preliminary version of this work
was presented by \citet{linderman2015cosyne}.

Chapter~\ref{chap:six} continues the network theme, but here we begin
to transition from static models to ones with dynamic latent
states. Specifically, we adopt a mechanistic view of the network and
treat its connections as actual synapses.  This leads to a
consideration of the evolution of synaptic weights over time.
%From this perspective, it is natural to consider dynamic learning rules 
%that govern how synaptic weights evolve over time.
We construct a framework for modeling arbitrary synaptic plasticity rules
and fitting them with particle MCMC.  This chapter is based on
\citet{linderman2014framework}.

Chapter~\ref{chap:seven} departs from network models and instead
considers dynamical discrete state space models, specifically hidden Markov
models (HMM), for neural data. We address a major challenge in
applying these well-known models, namely, selecting the number of
latent states. By introducing a hierarchical Dirichlet process (HDP)
prior and both an MCMC and a variational inference algorithm, we
develop a Bayesian nonparametric solution to this problem.  While this
combination is relatively well studied, these models are notoriously
sensitive to hyperparameter settings. Thus, we introduce a variety of
methods for selecting hyperparameters and perform a thorough
comparison on real and synthetic data. This is based on
\citet{linderman2016nonparametric}.

Building on the discrete state space models of the preceding chapter,
Chapter~\ref{chap:eight} develops efficient Bayesian inference
algorithms for switching linear dynamical systems (SLDSs) with
discrete count observations. The \polyagamma augmentation once again
makes Bayesian inference surprisingly easy. Given these auxiliary
variables, a host of tools for inference in Gaussian models is at
our disposal.  We then turn to a major problem, that of model comparison.
We derive a novel sampling algorithm for the \polyagamma distribution
that renders annealed importance sampling (AIS) simple and efficient
for a broad class of models, including the SLDSs of this chapter.
A preliminary version of this work was presented in \citet{linderman2016cosyne}.

Chapter~\ref{chap:nine} takes a step in a radical direction. We build
on the probabilistic models of the preceding chapters, but here we
combine these models with a top-down theory of neural
computation. Specifically, we consider the repercussions of the
``Bayesian brain'' hypothesis, namely, that neural circuits are
performing approximate Bayesian inference in a probabilistic model of
the world. We make predictions about the patterns of neural spike
trains that would be expected from such a circuit, and show how the
methods of previous chapters can be leveraged to reverse engineer
probabilistic models from observations of neural spike trains. This
work is necessarily speculative, but we believe it suggests a
promising path forward as we seek to reconcile computational and
algorithmic theories of neural computation with large scale
recordings.

Code for these models and inference algorithms, as well as for many of the
figures in this thesis, is available at \url{https://github.com/slinderman}.
